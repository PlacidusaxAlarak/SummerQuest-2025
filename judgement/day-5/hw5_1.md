# Day-5 作业1：Transformer FFN 并行策略分析

作者：殷林琪

## 题目描述

Transformer 的 FFN模块由两个线性层组成，结构为：

```python
Input (X) ∈ ℝ^{B × S × d}
↓
Linear1: W1 ∈ ℝ^{d × 4d}      → GeLU
↓
Linear2: W2 ∈ ℝ^{4d × d}
↓
Output (Z) ∈ ℝ^{B × S × d}
```

### 模型配置

Batch size: $B = 8$
Sequence length: $S = 128$
Hidden dim: $d = 1024$
使用TP，并行度为 $P = 4$

## 1. 并行策略与张量 Shape 分析

### 1.1 Linear1 使用 Column Parallel

#### 1.1.1 权重矩阵 Shape
每个 rank 上的权重矩阵 $W_1^{(i)}$ shape:

$$W_1^{(i)} \in \mathbb{R}^{1024 \times (4096 / 4)} = \mathbb{R}^{1024 \times 1024}$$

#### 1.1.2 输入张量 Shape
每个 rank 输入的张量 $X$ 的 shape 是:

$$X \in \mathbb{R}^{8 \times 128 \times 1024} \quad \text{（所有 rank 都需要完整 X）}$$

#### 1.1.3 输出张量 Shape
每个 rank 本地输出 $Y_i$的 shape 是？最终如何得到完整的 $Y$:

本地输出： $$Y_i = X \cdot W_1^{(i)} \in \mathbb{R}^{8 \times 128 \times 1024}$$

完整： $$Y = \text{Concat}(Y_0, Y_1, Y_2, Y_3) \in \mathbb{R}^{8 \times 128 \times 4096}$$

### 1.2 Linear2 使用 Row Parallel

#### 1.2.1 权重矩阵 Shape
每个 rank 上的权重矩阵 $W_2^{(i)}$ shape 是:

$$W_2^{(i)} \in \mathbb{R}^{(4096 / 4) \times 1024} = \mathbb{R}^{1024 \times 1024}$$

#### 1.2.2 输入张量 Shape
每个 rank 接受输入的张量 的 shape 是: 

$$ Y_i \in \mathbb{R}^{8 \times 128 \times 1024}$$

#### 1.2.3 输出张量 Shape
每个 rank 本地输出 $Z_i$的 shape 是？最终如何得到完整的 $Z$:

$$ Z_i = Y_i \cdot W_2^{(i)} \in \mathbb{R}^{8 \times 128 \times 1024}$$

$$Z = \sum_{i=1}^4 Z_i \in \mathbb{R}^{8 \times 128 \times 1024}$$

## 2. 通信分析

### 2.1 Linear1 通信分析

#### 2.1.1 前向传播通信
前向传播是否需要通信？通信操作是？通信量是多少？ 

所有 rank 使用完整输入 $X$，本地计算 $Y_i$，无需通信

最终拼接 $Y = \text{Concat}(Y_i)$：通常在下一层做 RowParallel 时分片使用，没有显式 all-gather 通信，通信量为0

#### 2.1.2 反向传播通信
反向传播时，计算 $\partial L / \partial X$ 是否需要通信？说明原因。

每个 rank 的 $W_1^{(i)}$和 $\partial L / \partial Y_i$仅覆盖部分输出

为计算全量 $\partial L / \partial X$，需： $$\text{All-Reduce} \left( \sum_{i=1}^P \partial L / \partial X_i \right)$$

通信方法为 All-Reduce，需要通信的数据量为： $B \times S \times d = 8 \times 128 \times 1024 = 1\,048\,576$个 float（约 4MB）

**Ring算法下的实际物理流量分析：**

Ring All-Reduce算法分为两个阶段：
1. **Reduce-Scatter阶段**：将数据分成P份，每个节点负责聚合其中一份
2. **All-Gather阶段**：将聚合结果分发到所有节点

对于P=4个节点的Ring拓扑：
- 每个阶段需要(P-1) = 3轮通信
- 每轮每个节点发送数据量的1/P = 1/4
- 总共需要2×3 = 6轮通信

**实际物理流量计算：**
- 每个节点的发送量：$6 \times \frac{1\,048\,576}{4} = 6 \times 262\,144 = 1\,572\,864$个float
- 全网络总流量：$4 \times 1\,572\,864 = 6\,291\,456$个float（约24MB）
- 物理流量放大倍数：$\frac{6\,291\,456}{1\,048\,576} = 6$倍


### 2.2 Linear2 通信分析

#### 2.2.1 前向传播通信
前向传播是否需要通信？通信操作是？通信量是多少？ 

每个 rank 仅有部分输入 $Y_i$，需要聚合所有本地输出 $Z_i$，最终输出： $$\text{All-Reduce} \left( \sum_{i=1}^P Z_i \right)$$

通信方法为 All-Reduce，需要通信的数据量为 $8 \times 128 \times 1024 = 1\,048\,576$个 float（约 4MB）

**Ring算法下的实际物理流量分析：**

Ring All-Reduce算法的通信模式与2.1.2相同：
- Reduce-Scatter阶段：3轮通信，每轮发送1/4数据
- All-Gather阶段：3轮通信，每轮发送1/4数据
- 总计6轮通信

**实际物理流量计算：**
- 每个节点的发送量：$6 \times \frac{1\,048\,576}{4} = 1\,572\,864$个float
- 全网络总流量：$4 \times 1\,572\,864 = 6\,291\,456$个float（约24MB）
- 物理流量放大倍数：6倍

#### 2.2.2 反向传播通信
反向传播时，计算 $\partial L / \partial X$ 是否需要通信？说明原因。

每个 rank 局部计算本地反向传播，可保持输入分片，无需通信，通信量为0

## 3. 并行策略组合分析

### 3.1 两层都使用 Row Parallel 的问题

Linear1 输出是根据输入的分片输出，但 Linear2 需要全量输入，则需要 All-Gather，导致增加显式通信开销

### 3.2 两层都使用 Column Parallel 的问题

Linear1 输出是拼接的，Linear2 的输入却被分片，Linear2 前向需显式 All-to-All 或 Scatter，反向也需要通信